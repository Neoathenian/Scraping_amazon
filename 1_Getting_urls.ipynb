{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This script´s goal is to gather information for a certain product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) First we´ll get the links of a lot of these products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup  # Import BeautifulSoup\n",
    "\n",
    "\n",
    "# Configure Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--headless\")  # Run headless (without opening browser)\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# URL of the Amazon search page\n",
    "url = \"https://www.amazon.es\"\n",
    "\n",
    "# Go to the Amazon search page\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product to search\n",
    "wait = WebDriverWait(driver, 10)  # Timeout after 10 seconds\n",
    "search_box = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"twotabsearchtextbox\"]')))\n",
    "search_box.send_keys(\"aspiradora inalámbrica\")  # Enter the search term\n",
    "search_box.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 products on the page, and missed 0 products.\n",
      "Found 49 products on the page, and missed 0 products.\n",
      "Found 49 products on the page, and missed 0 products.\n",
      "Found 49 products on the page, and missed 0 products.\n",
      "Found 49 products on the page, and missed 0 products.\n",
      "Found 49 products on the page, and missed 0 products.\n",
      "Found 19 products on the page, and missed 0 products.\n",
      "Next button not found or disabled, stopping pagination.\n",
      "Total products collected: 314\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "\n",
    "# Function to navigate through Amazon search pages and extract data\n",
    "def scrape_amazon_pages(driver, base_url,max_pages=5):\n",
    "    page = 1\n",
    "    product_infos = []\n",
    "    missed_products=[]\n",
    "\n",
    "    while page <= max_pages:\n",
    "        # Extract product information from the current page\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        products = soup.find_all('div', {'data-cy': 'title-recipe'})\n",
    "\n",
    "\n",
    "        initial_length = len(product_infos)\n",
    "        initial_missed_length = len(missed_products)\n",
    "        for product in products:\n",
    "            try:\n",
    "                # Title and link extraction as in the previous code\n",
    "                title_span = product.select_one(\"span[class*='a-text-normal']\")\n",
    "                title = title_span.get_text(strip=True) if title_span else \"Title Not Found\"\n",
    "                \n",
    "                link_tag = product.select_one(\"a[href*='/dp/']\")\n",
    "                link = link_tag['href'] if link_tag else None\n",
    "                full_link = f\"{base_url}{link}\" if \"https://\" not in link else (link if link else \"Link Not Found\")\n",
    "\n",
    "                if title != \"Title Not Found\" and link:\n",
    "                    product_infos.append({\"title\": title, \"link\": full_link})\n",
    "\n",
    "            except AttributeError:\n",
    "                missed_products.append(product)\n",
    "            \n",
    "        print(f\"Found {len(product_infos) - initial_length} products on the page, and missed {len(missed_products) - initial_missed_length} products.\")\n",
    "\n",
    "        if page < max_pages:\n",
    "            # Try to navigate to the next page\n",
    "            try:\n",
    "                next_button = driver.find_element(\"css selector\", \"a.s-pagination-next\")\n",
    "                driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                time.sleep(5)  # Wait for the page to load\n",
    "            except NoSuchElementException:\n",
    "                print(\"Next button not found or disabled, stopping pagination.\")\n",
    "                break\n",
    "        page += 1\n",
    "\n",
    "    return product_infos\n",
    "\n",
    "## Call the scraping function\n",
    "products_data = scrape_amazon_pages(driver,url, max_pages=50)\n",
    "print(\"Total products collected:\", len(products_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "os.makedirs(\"Databases\", exist_ok=True)\n",
    "products_information=pd.DataFrame(products_data)\n",
    "products_information.to_csv(\"Databases/products_information.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Now we´ll go to each of those pages and retrieve the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "products_information=pd.read_csv(\"Databases/products_information.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--headless\")  # Run headless (without opening browser)\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "save_dir=\"Webpages/Aspiradoras\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for title,link in zip(products_information[\"title\"], products_information[\"link\"]):\n",
    "    save_location=f\"{save_dir}/{title[:40].replace(\"/\",\"\").replace(\"|\",\"\")}.html\"\n",
    "    if os.path.exists(save_location):\n",
    "        continue\n",
    "    driver.get(link)\n",
    "    html_content = driver.page_source\n",
    "    with open(save_location, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_information.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scraping_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
